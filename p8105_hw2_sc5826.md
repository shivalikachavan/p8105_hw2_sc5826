p8105_hw2_sc5826
================
Shivalika Chavan
2025-09-23

``` r
library(tidyverse)
library(haven)
library(readxl)
# order of levels for making month into a factor so the months are in chronological order, not alphabetical
month_order = c("January","February","March","April","May","June","July","August","September","October","November","December") 
```

## Problem 1

Cleaning `pols_month.csv`

``` r
pols_month = read.csv("./data/fivethirtyeight_datasets/pols-month.csv") |> 
  janitor::clean_names() |> 
  separate("mon", into = c("year", "month", "day"), sep = "-", convert = TRUE) |> 
  mutate(
    month =
      recode(
        month,
        `1` = "January",
        `2` = "February",
        `3` = "March",
        `4` = "April",
        `5` = "May",
        `6` = "June",
        `7` = "July",
        `8` = "August",
        `9` = "September",
        `10` = "October",
        `11` = "November",
        `12` = "December"
        ),
    month = factor(month, levels = month_order), 
    president = 
      recode(
        prez_dem, # using prez_dem to create president since prez_gop has erroneous values of 2
        `0` = "gop",
        `1` = "dem"
      )
  ) |> 
  select(-day, -prez_dem, -prez_gop) |> # removing day, prez_dem, and prez_gop
  arrange(year, month) |> #arranging by year and month
  relocate(year, month, everything()) # moving year and month to front
```

Cleaning `snp.csv`. The `year` column is either `> 15` or `< 50`. I’m
making the assumption that all the years `> 15` are in the 20th century
(adding 1900) and all the years `< 50` are in the 21st century (adding
2000). Doing this makes the year column consistent across the
`pols_month` and `snp` data frames.

``` r
snp = read_csv("./data/fivethirtyeight_datasets/snp.csv") |> 
  janitor::clean_names() |> 
  separate("date", into = c("month", "day", "year"), sep = "/", convert = TRUE) |> 
  mutate(
    month =
      recode(
        month,
        `1` = "January",
        `2` = "February",
        `3` = "March",
        `4` = "April",
        `5` = "May",
        `6` = "June",
        `7` = "July",
        `8` = "August",
        `9` = "September",
        `10` = "October",
        `11` = "November",
        `12` = "December"
        ),
    month = factor(month, levels = month_order),
    year = 
      case_when(
        year > 15 ~ year + 1900,
        .default = year + 2000
      )
  ) |> 
  select(-day) |> # removing day
  arrange(year, month) |> #arranging by year and month
  relocate(year, month, everything()) # moving year and month to front
```

    ## Rows: 787 Columns: 2
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (1): date
    ## dbl (1): close
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

Cleaning `unemployment.csv`.

``` r
unemployment = read_csv("./data/fivethirtyeight_datasets/unemployment.csv") |> 
  janitor::clean_names() |> 
  pivot_longer(
    jan:dec,
    names_to = "month",
    values_to = "unemployment_pct"
  ) |> 
  mutate(
    month =
      recode(
        month,
        `jan` = "January",
        `feb` = "February",
        `mar` = "March",
        `apr` = "April",
        `may` = "May",
        `jun` = "June",
        `jul` = "July",
        `aug` = "August",
        `sep` = "September",
        `oct` = "October",
        `nov` = "November",
        `dec` = "December"
        ),
    month = factor(month, levels = month_order),
  ) |> 
  arrange(year, month) |> #arranging by year and month
  relocate(year, month, everything()) # moving year and month to front
```

    ## Rows: 68 Columns: 13
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (13): Year, Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

First merging `snp` into `pols_month`, then `unemployment` into the
result using left_joins. This keeps all the observations in
`pols_month`, since it includes some months and years during which there
is no data recorded in `snp` or `unemployment.`

``` r
pols_snp_unemployment_df = left_join(pols_month, snp, by = c("year", "month")) |> 
  left_join(unemployment, by = c("year", "month"))
```

The cleaned `pols_month` data set has 822 observations and 9 variables
(year, month, gov_gop, sen_gop, rep_gop, gov_dem, sen_dem, rep_dem,
president), ranging from 1947 to 2015. The cleaned `snp` data set has
787 observations and 3 variables (year, month, close), ranging from 1950
to 2015. The cleaned `unemployment` data set has 816 observations and 3
variables (year, month, unemployment_pct), ranging from 1948 to 2015.

The resulting data set, `pols_snp_unemployment_df`, has 822 observations
and 11 variables (year, month, gov_gop, sen_gop, rep_gop, gov_dem,
sen_dem, rep_dem, president, close, unemployment_pct), ranging from 1947
to 2015. This data set contains information about the \# of GOP and
Democrat governors, senators, representatives (congresspeople), the
political party of the president, the closing value of the S&P stock
index, and unemployment percentage for each month from 1947 to 2015.
Some data about the S&P stock index closing and unemployment percentage
was not collected for older months, and is marked as `NA` in these
cases.

## Problem 2

Reading and cleaning the `Mr. Trash Wheel` data set. Since the given
month is not a factor, I’m using the same recoding pipeline as Problem
1, using `date` as the ground truth and getting rid of the original
month and year variables. I’m also excluding the last row (row 655 in
the original Excel sheet), which contains the sum of each column. These
values can always be recalculated using the R data frame.

``` r
mr_trash_wheel_data = 
  read_excel("./data/202409_Trash_Wheel_Collection_Data.xlsx", sheet = "Mr. Trash Wheel", range = "A2:N653") |> 
  janitor::clean_names() |> 
  select(-month, -year) |> 
  separate("date", into = c("year", "month", "day"), sep = "-", convert = TRUE) |> 
  mutate(
    month =
      recode(
        month,
        `1` = "January",
        `2` = "February",
        `3` = "March",
        `4` = "April",
        `5` = "May",
        `6` = "June",
        `7` = "July",
        `8` = "August",
        `9` = "September",
        `10` = "October",
        `11` = "November",
        `12` = "December"
        ),
    month = factor(month, levels = month_order),
    sports_balls = as.integer(round(sports_balls, digits = 0)),
    trash_wheel = "Mr. Trash Wheel"
  )|> 
  relocate(glass_bottles, sports_balls, .after = "trash_wheel")
```

Reading and cleaning the `Professor Trash Wheel` data set. Same
justifications for getting rid of original month and year columns and
row 123 which has sums.

``` r
prof_trash_wheel_data = 
  read_excel("./data/202409_Trash_Wheel_Collection_Data.xlsx", sheet = "Professor Trash Wheel", range = "A2:M120") |> 
  janitor::clean_names() |> 
  select(-month, -year) |> 
  separate("date", into = c("year", "month", "day"), sep = "-", convert = TRUE) |> 
  mutate(
    month =
      recode(
        month,
        `1` = "January",
        `2` = "February",
        `3` = "March",
        `4` = "April",
        `5` = "May",
        `6` = "June",
        `7` = "July",
        `8` = "August",
        `9` = "September",
        `10` = "October",
        `11` = "November",
        `12` = "December"
        ),
    month = factor(month, levels = month_order),
    trash_wheel = "Professor Trash Wheel"
  ) |> 
  relocate(glass_bottles, .after = "trash_wheel")
```

Reading and cleaning the `Gwynnda Trash Wheel` data set. Same
justifications for getting rid of original month and year columns and
row 266 which has sums.

``` r
gwynnda_trash_wheel_data = 
  read_excel("./data/202409_Trash_Wheel_Collection_Data.xlsx", sheet = "Gwynnda Trash Wheel", range = "A2:L265") |> 
  janitor::clean_names() |> 
  select(-month, -year) |> 
  separate("date", into = c("year", "month", "day"), sep = "-", convert = TRUE) |> 
  mutate(
    month =
      recode(
        month,
        `1` = "January",
        `2` = "February",
        `3` = "March",
        `4` = "April",
        `5` = "May",
        `6` = "June",
        `7` = "July",
        `8` = "August",
        `9` = "September",
        `10` = "October",
        `11` = "November",
        `12` = "December"
        ),
    month = factor(month, levels = month_order),
    trash_wheel = "Gwynnda Trash Wheel"
  )
```

Combining the data sets, moving the dates and `trash_wheel` name to the
front of the resulting data set, arranging all observations
chronological order and `trash_wheel` name.

``` r
trash_wheel_data = bind_rows(mr_trash_wheel_data, prof_trash_wheel_data, gwynnda_trash_wheel_data) |> 
  relocate(year, month, day, trash_wheel) |> 
  arrange(year, month, day, trash_wheel)
```

In `mr_trash_wheel_data`, there are 651 observations and 651 unique
dumpsters. In `prof_trash_wheel_data`, there are 118 observations and
118 unique dumpsters. In `gwynnda_trash_wheel_data`, there are 263
observations and only 262 unique dumpsters. This is unlike the other 2
datasets, which have 1 observation for each dumpster. Looking closer at
`gwynnda_trash_wheel_data`, dumpster 21 has 2 observations.

``` r
#Doing calculations here to keep the inline R code relatively clean. 
total_weight_tons_prof_trash_wheel = trash_wheel_data |> 
  filter(trash_wheel == "Professor Trash Wheel") |> 
  select(weight_tons) |> 
  sum() |> 
  as.character()

total_cigarette_butts_gwynnda = trash_wheel_data |> 
  filter(trash_wheel == "Gwynnda Trash Wheel", year == 2022, month == "June") |> 
  select(cigarette_butts) |> 
  sum() |> 
  as.character()
```

The combined data set `trash_wheel_data` has 1032 observations and 15
variables. Each unique observation is noted by the date (`year`,
`month`, and `day`), the name of the `trash_wheel` (Mr. Trash Wheel,
Professor Trash Wheel, Gwynnda Trash Wheel), and the `dumpster` number.
Some observations have missing data for the remaining variables
(e.g. `plastic_bottles`, `polystyrene`, `cigarette_butts`,
`plastic_bags`, `wrappers`, `homes_powered`, `glass_bottles`,
`sports_balls`). Professor Trash Wheel collected 246.74 tons of trash in
total. The original excel data omits the last 2 rows in the calculated
sum of 241.26, which is why the sums are different. In June of 2022,
Gwynnda collected 18120 cigarette butts.

## Problem 3

Cleaning the `zip_codes_nyc` data set. FIPS is Federal Information
Processing System Codes for States and Counties.

``` r
zip_codes_nyc = read.csv("./data/zillow_data/zip_codes.csv") |> 
  janitor::clean_names()|> 
  select(-state_fips, -file_date) |> 
  relocate(county_code, county_fips, county)
```

It makes sense that the state FIPS code is the same for all zip codes in
the state of NYC (all are in New York). I verified this by looking at
all of the unique `state_fips` (there is 1). It also happens to be that
every observation has the same `file_date` of `7/25/07`. After a brief
search on Google, I’m assuming that these zip codes were filed in the
early days of Zillow, which launched in 2006. These variables are not
really relevant to the data set, so I’m going to exclude them from the
working data set. I’m also moving the county code/FIPS and county name
to the front of the data set to have the more broad categories on the
left.

What’s interesting with this data set is that while there are 322
observations, but only 320 unique zip codes. This means that there are 2
zip codes that are repeated in the data set.

``` r
repeated_zip_codes = zip_codes_nyc |> 
  group_by(zip_code) |> 
  count(zip_code) |> 
  filter(n > 1) |> 
  pull(zip_code)
```

The two repeated zip codes are 10463, 11201.

``` r
zip_codes_nyc |> 
  filter(zip_code %in% repeated_zip_codes) |> 
  arrange(zip_code)
##   county_code county_fips   county zip_code              neighborhood
## 1           5       36005    Bronx    10463 Kingsbridge and Riverdale
## 2          61       36061 New York    10463 Kingsbridge and Riverdale
## 3          47       36047    Kings    11201        Northwest Brooklyn
## 4          61       36061 New York    11201        Northwest Brooklyn
```

The zipcode 10463 for the Kingsbridge and Riverdale neighborhood appears
twice, once in the Bronx and once in New York. The zipcode 11201 for the
Northwest Brooklyn neighborhood appears twice, once in Kings and once in
New York.

Moving onto the Zillow Observed Rent Indices (ZORI) index data set.

``` r
zori_nyc = read.csv("./data/zillow_data/Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv") |>
  janitor::clean_names() |>
  pivot_longer(
    x2015_01_31:x2024_08_31,
    names_to = "date",
    names_prefix = "x",
    values_to = "index"
  ) |>
  separate("date", into = c("year", "month", "day"), sep = "_", convert = TRUE) |>
  mutate(
    month =
      recode(
        month,
        `1` = "January",
        `2` = "February",
        `3` = "March",
        `4` = "April",
        `5` = "May",
        `6` = "June",
        `7` = "July",
        `8` = "August",
        `9` = "September",
        `10` = "October",
        `11` = "November",
        `12` = "December"
        ),
    month = factor(month, levels = month_order),
    county = str_replace(county_name, " County", ""),
    zip_code = region_name
  ) |>
  select(-county_name, -region_type, -region_name, -state_name, -state, -city, -metro) |> 
  relocate(year, month, day, county, zip_code, region_id, size_rank, index)
```

First, I see that all of the dates with recorded ZORI is spread across a
bunch of columns. This can be consolidated into a single `date` variable
with the given index or `NA`. It looks like every `region_id` has some
missing values, so there will be lots of `NA` indices.

Like in previous problems, I’m separating the date into `year`, `month`,
and `day`, and setting month to be a factor so the default order is
chronological, not alphabetical.

In the formatting of the original data set, I saw that there are 149
observations and each observation corresponds to a unique `region_id`,
`size_rank`, `region_name`, and I want to keep these (or some version of
these) in my final data set.

The `county_name` variable lists the 5 counties in NYC as \_\_ County
(e.g. Kings County). To match the style for county names in the
`zip_codes_nyc` data set, I’m truncating “County” from each name.

Every `region_type` is `"zip"`, so I’m renaming `region_name` to
`zip_code` to match the style for the zip codes in the `zip_codes_nyc`
data set.

For studies looking at multiple states, cities, etc, the variables
`state_name`, `state`, `city`, and `metro` would be useful, but since
this study is focusing on New York City, keeping these variables seems
repetitive. By including `nyc` in the data frame names, I’m effectively
communicating the same information as these variables (i.e. someone
unfamiliar with the data would be able to tell that this is data from
New York City).

So now, we have `zip_codes_nyc`, which has information about counties in
NYC (and associated FIPS code and county code), zip codes, and
neighborhoods. It has 322 observations.

We also have `zori_nyc`, which also has information about counties in
NYC, the zip codes, region IDs, and size ranks, as well as the ZORI at
each date recorded. It has 17284 observations.

``` r
colnames(zip_codes_nyc)
## [1] "county_code"  "county_fips"  "county"       "zip_code"     "neighborhood"
colnames(zori_nyc)
## [1] "year"      "month"     "day"       "county"    "zip_code"  "region_id"
## [7] "size_rank" "index"
```

When it comes to joining these two data sets, I have to keep in mind
that simply joining on `zip_code` will not suffice, since 2 of the zip
codes show up in multiple counties. Since I am primarily interested in
the ZORI data (with additional information on neighborhood), a left join
on county and ZIP code makes the most sens.

``` r
zori_nyc_neighborhoods = left_join(zori_nyc, zip_codes_nyc, by = c("zip_code", "county")) |> 
  relocate(year, month, day, county, county_code, county_fips, neighborhood, size_rank, region_id, zip_code) |> 
  arrange(year, month, day)
colnames(zori_nyc_neighborhoods)
##  [1] "year"         "month"        "day"          "county"       "county_code" 
##  [6] "county_fips"  "neighborhood" "size_rank"    "region_id"    "zip_code"    
## [11] "index"
```

Looking at the final data set, we can see that there are 11 variables,
which corresponds to the 8 variables in `zori_nyc` plus the
`county_code`, `county_fips`, and `neighborhood` variables found in
`zip_codes_nyc` data set. The resulting data set is set in chronological
order (putting the date-related variables at the beginning). After the
date variables comes the variables related to the county (the name,
code, and FIPS code). After county comes the neighborhood, followed by
zip code related variables (size_rank, ID, zip code). The size rank is
the city’s rank by population size.

There are 17284 total observations in `zori_nyc_neighborhoods`, with 149
unique ZIP codes and 43 unique neighborhoods.

``` r
zip_codes_not_in_zori = anti_join(zip_codes_nyc, zori_nyc, by = c("county", "zip_code")) 

codes_with_neighborhood_not_in_zori = zip_codes_not_in_zori |> 
  filter(!is.na(neighborhood))
bronx_neighborhoods = codes_with_neighborhood_not_in_zori |> filter(county == "Bronx") 
kings_neighborhoods = codes_with_neighborhood_not_in_zori |> filter(county == "Kings") 
newyork_neighborhoods = codes_with_neighborhood_not_in_zori |> filter(county == "New York") 
queens_neighborhoods = codes_with_neighborhood_not_in_zori |> filter(county == "Queens") 
richmond_neighborhoods = codes_with_neighborhood_not_in_zori |> filter(county == "Richmond") 
```

There are 174 ZIP codes that appear in `zip_codes_nyc` but not
`zori_nyc`. Of these, 137 ZIP codes have no associated neighborhood. The
remaining 37 ZIP codes are in 16 distinct neighborhoods across all 5
counties:

- **Bronx** - Southeast Bronx, Hunts Point and Mott Haven, Northeast
  Bronx
- **Kings** - Southern Brooklyn, Canarsie and Flatlands, Rockaways
- **New York** - Chelsea and Clinton, Kingsbridge and Riverdale,
  Northwest Brooklyn
- **Queens** - Southeast Queens, North Queens, Northeast Queens,
  Jamaica, Southwest Queens, Rockaways
- **Richmond** - Port Richmond, South Shore

# Add a few illustrative examples discuss why these ZIP codes might be excluded from the Zillow dataset.

``` r
top10_rental_price_change = zori_nyc_neighborhoods |> 
  filter(month == "January", year %in% c(2020, 2021), !is.na(index), !is.na(neighborhood)) |>
  group_by(zip_code) |> 
  filter(n() == 2) |> 
  mutate(
    date = paste(tolower(month), day, year, sep = "_")
  ) |> 
  select(-year, -month, -day) |> 
  pivot_wider(
    names_from = "date", 
    values_from = "index"
  ) |> 
  summarize(
    county = county,
    neighborhood = neighborhood,
    zip_code = zip_code,
    index_change_covid = january_31_2021 - january_31_2020
  ) |> 
  arrange(index_change_covid) |> 
  slice(1:10) 
knitr::kable(top10_rental_price_change)
```

| zip_code | county   | neighborhood                  | index_change_covid |
|---------:|:---------|:------------------------------|-------------------:|
|    10007 | New York | Lower Manhattan               |          -912.5966 |
|    10009 | New York | Lower East Side               |          -714.2550 |
|    10016 | New York | Gramercy Park and Murray Hill |          -711.7045 |
|    10001 | New York | Chelsea and Clinton           |          -710.4499 |
|    10002 | New York | Lower East Side               |          -710.3028 |
|    10004 | New York | Lower Manhattan               |          -705.9608 |
|    10038 | New York | Lower Manhattan               |          -697.5853 |
|    10012 | New York | Greenwich Village and Soho    |          -686.2218 |
|    10010 | New York | Gramercy Park and Murray Hill |          -684.9304 |
|    10003 | New York | Lower East Side               |          -672.5404 |

All of the biggest drops are in New York County, specifically in
downtown Manhattan. The neighborhoods are Lower Manhattan, Lower East
Side, Gramercy Park and Murray Hill, Chelsea and Clinton, Greenwich
Village and Soho. These are all typically more expensive and have a
large amount of rental properties (as compared to other neighborhoods
that will have more properties that are occupied by the owner of the
property)
