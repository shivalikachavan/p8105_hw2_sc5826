---
title: "p8105_hw2_sc5826"
author: "Shivalika Chavan"
date: "2025-09-23"
output: github_document
---
```{r message=FALSE}
library(tidyverse)
library(haven)
library(readxl)
# order of levels for making month into a factor so the months are in chronological order, not alphabetical
month_order = c("January","February","March","April","May","June","July","August","September","October","November","December") 
```

## Problem 1
Cleaning `pols_month.csv`
```{r}
pols_month = read.csv("./data/fivethirtyeight_datasets/pols-month.csv") |> 
  janitor::clean_names() |> 
  separate("mon", into = c("year", "month", "day"), sep = "-", convert = TRUE) |> 
  mutate(
    month =
      recode(
        month,
        `1` = "January",
        `2` = "February",
        `3` = "March",
        `4` = "April",
        `5` = "May",
        `6` = "June",
        `7` = "July",
        `8` = "August",
        `9` = "September",
        `10` = "October",
        `11` = "November",
        `12` = "December"
        ),
    month = factor(month, levels = month_order), 
    president = 
      recode(
        prez_dem, # using prez_dem to create president since prez_gop has erroneous values of 2
        `0` = "gop",
        `1` = "dem"
      )
  ) |> 
  select(-day, -prez_dem, -prez_gop) |> # removing day, prez_dem, and prez_gop
  arrange(year, month) |> #arranging by year and month
  relocate(year, month, everything()) # moving year and month to front
```

Cleaning `snp.csv`. The `year` column is either `> 15` or `< 50`. I'm making the assumption that all the years `> 15` are in the 20th century (adding 1900) and all the years `< 50` are in the 21st century (adding 2000). Doing this makes the year column consistent across the `pols_month` and `snp` data frames. 
```{r message = FALSE}
snp = read_csv("./data/fivethirtyeight_datasets/snp.csv", show_col_types = FALSE) |> 
  janitor::clean_names() |> 
  separate("date", into = c("month", "day", "year"), sep = "/", convert = TRUE) |> 
  mutate(
    month =
      recode(
        month,
        `1` = "January",
        `2` = "February",
        `3` = "March",
        `4` = "April",
        `5` = "May",
        `6` = "June",
        `7` = "July",
        `8` = "August",
        `9` = "September",
        `10` = "October",
        `11` = "November",
        `12` = "December"
        ),
    month = factor(month, levels = month_order),
    year = 
      case_when(
        year > 15 ~ year + 1900,
        .default = year + 2000
      )
  ) |> 
  select(-day) |> # removing day
  arrange(year, month) |> #arranging by year and month
  relocate(year, month, everything()) # moving year and month to front
```

Cleaning `unemployment.csv`.
```{r}
unemployment = read_csv("./data/fivethirtyeight_datasets/unemployment.csv", show_col_types = FALSE) |> 
  janitor::clean_names() |> 
  pivot_longer(
    jan:dec,
    names_to = "month",
    values_to = "unemployment_pct"
  ) |> 
  mutate(
    month =
      recode(
        month,
        `jan` = "January",
        `feb` = "February",
        `mar` = "March",
        `apr` = "April",
        `may` = "May",
        `jun` = "June",
        `jul` = "July",
        `aug` = "August",
        `sep` = "September",
        `oct` = "October",
        `nov` = "November",
        `dec` = "December"
        ),
    month = factor(month, levels = month_order),
  ) |> 
  arrange(year, month) |> #arranging by year and month
  relocate(year, month, everything()) # moving year and month to front
```
First merging `snp` into `pols_month`, then `unemployment` into the result using left_joins. This keeps all the observations in `pols_month`, since it includes some months and years during which there is no data recorded in `snp` or `unemployment.` 
```{r}
pols_snp_unemployment_df = left_join(pols_month, snp, by = c("year", "month")) |> 
  left_join(unemployment, by = c("year", "month"))
```

The cleaned `pols_month` data set has `r nrow(pols_month)` observations and `r ncol(pols_month)` variables (`r colnames(pols_month)`), ranging from `r min(pull(pols_month, year))` to `r max(pull(pols_month, year))`. The cleaned `snp` data set has `r nrow(snp)` observations and `r ncol(snp)` variables (`r colnames(snp)`), ranging from `r min(pull(snp, year))` to `r max(pull(snp, year))`. The cleaned `unemployment` data set has `r nrow(unemployment)` observations and `r ncol(unemployment)` variables (`r colnames(unemployment)`), ranging from `r min(pull(unemployment, year))` to `r max(pull(unemployment, year))`. 

The resulting data set, `pols_snp_unemployment_df`, has `r nrow(pols_snp_unemployment_df)` observations and `r ncol(pols_snp_unemployment_df)` variables (`r colnames(pols_snp_unemployment_df)`), ranging from `r min(pull(pols_snp_unemployment_df, year))` to `r max(pull(pols_snp_unemployment_df, year))`. This data set contains information about the # of GOP and Democrat governors, senators, representatives (congresspeople), the political party of the president, the closing value of the S&P stock index, and unemployment percentage for each month from `r min(pull(pols_snp_unemployment_df, year))` to `r max(pull(pols_snp_unemployment_df, year))`. Some data about the S&P stock index closing and unemployment percentage was not collected for older months, and is marked as `NA` in these cases. 




## Problem 2
Reading and cleaning the `Mr. Trash Wheel` data set. Since the given month is not a factor, I'm using the same recoding pipeline as Problem 1, using `date` as the ground truth and getting rid of the original month and year variables. I'm also excluding the last row (row 655 in the original Excel sheet), which contains the sum of each column. These values can always be recalculated using the R data frame. 
```{r}
mr_trash_wheel_data = 
  read_excel("./data/202409_Trash_Wheel_Collection_Data.xlsx", sheet = "Mr. Trash Wheel", range = "A2:N653") |> 
  janitor::clean_names() |> 
  select(-month, -year) |> 
  separate("date", into = c("year", "month", "day"), sep = "-", convert = TRUE) |> 
  mutate(
    month =
      recode(
        month,
        `1` = "January",
        `2` = "February",
        `3` = "March",
        `4` = "April",
        `5` = "May",
        `6` = "June",
        `7` = "July",
        `8` = "August",
        `9` = "September",
        `10` = "October",
        `11` = "November",
        `12` = "December"
        ),
    month = factor(month, levels = month_order),
    sports_balls = as.integer(round(sports_balls, digits = 0)),
    trash_wheel = "Mr. Trash Wheel"
  )|> 
  relocate(glass_bottles, sports_balls, .after = "trash_wheel")
```

Reading and cleaning the `Professor Trash Wheel` data set. Same justifications for getting rid of original month and year columns and row 123 which has sums. 
```{r}
prof_trash_wheel_data = 
  read_excel("./data/202409_Trash_Wheel_Collection_Data.xlsx", sheet = "Professor Trash Wheel", range = "A2:M120") |> 
  janitor::clean_names() |> 
  select(-month, -year) |> 
  separate("date", into = c("year", "month", "day"), sep = "-", convert = TRUE) |> 
  mutate(
    month =
      recode(
        month,
        `1` = "January",
        `2` = "February",
        `3` = "March",
        `4` = "April",
        `5` = "May",
        `6` = "June",
        `7` = "July",
        `8` = "August",
        `9` = "September",
        `10` = "October",
        `11` = "November",
        `12` = "December"
        ),
    month = factor(month, levels = month_order),
    trash_wheel = "Professor Trash Wheel"
  ) |> 
  relocate(glass_bottles, .after = "trash_wheel")
```

Reading and cleaning the `Gwynnda Trash Wheel` data set. Same justifications for getting rid of original month and year columns and row 266 which has sums. 
```{r}
gwynnda_trash_wheel_data = 
  read_excel("./data/202409_Trash_Wheel_Collection_Data.xlsx", sheet = "Gwynnda Trash Wheel", range = "A2:L265") |> 
  janitor::clean_names() |> 
  select(-month, -year) |> 
  separate("date", into = c("year", "month", "day"), sep = "-", convert = TRUE) |> 
  mutate(
    month =
      recode(
        month,
        `1` = "January",
        `2` = "February",
        `3` = "March",
        `4` = "April",
        `5` = "May",
        `6` = "June",
        `7` = "July",
        `8` = "August",
        `9` = "September",
        `10` = "October",
        `11` = "November",
        `12` = "December"
        ),
    month = factor(month, levels = month_order),
    trash_wheel = "Gwynnda Trash Wheel"
  )
```

Combining the data sets, moving the dates and `trash_wheel` name to the front of the resulting data set, arranging all observations chronological order and `trash_wheel` name. 
```{r}
trash_wheel_data = bind_rows(mr_trash_wheel_data, prof_trash_wheel_data, gwynnda_trash_wheel_data) |> 
  relocate(year, month, day, trash_wheel) |> 
  arrange(year, month, day, trash_wheel)
```

In `mr_trash_wheel_data`, there are `r nrow(mr_trash_wheel_data)` observations and `r length(unique(pull(mr_trash_wheel_data,dumpster)))` unique dumpsters. In `prof_trash_wheel_data`, there are `r nrow(prof_trash_wheel_data)` observations and `r length(unique(pull(prof_trash_wheel_data,dumpster)))` unique dumpsters. In `gwynnda_trash_wheel_data`, there are `r nrow(gwynnda_trash_wheel_data)` observations and only `r length(unique(pull(gwynnda_trash_wheel_data,dumpster)))` unique dumpsters. This is unlike the other 2 datasets, which have 1 observation for each dumpster. Looking closer at `gwynnda_trash_wheel_data`, dumpster 21 has `r sum(pull(gwynnda_trash_wheel_data, dumpster) == 21)` observations.


```{r echo=TRUE}
#Doing calculations here to keep the inline R code relatively clean. 
total_weight_tons_prof_trash_wheel = trash_wheel_data |> 
  filter(trash_wheel == "Professor Trash Wheel") |> 
  select(weight_tons) |> 
  sum() |> 
  as.character()

total_cigarette_butts_gwynnda = trash_wheel_data |> 
  filter(trash_wheel == "Gwynnda Trash Wheel", year == 2022, month == "June") |> 
  select(cigarette_butts) |> 
  sum() |> 
  as.character()
```


The combined data set `trash_wheel_data` has `r nrow(trash_wheel_data)` observations and `r ncol(trash_wheel_data)` variables. Each unique observation is noted by the date (`year`, `month`, and `day`), the name of the `trash_wheel` (`r unique(pull(trash_wheel_data, trash_wheel))`), and the `dumpster` number. Some observations have missing data for the remaining variables (e.g. `plastic_bottles`, `polystyrene`, `cigarette_butts`, `plastic_bags`, `wrappers`, `homes_powered`, `glass_bottles`, `sports_balls`). Professor Trash Wheel collected `r total_weight_tons_prof_trash_wheel` tons of trash in total. The original excel data omits the last 2 rows in the calculated sum of 241.26, which is why the sums are different. In June of 2022, Gwynnda collected `r total_cigarette_butts_gwynnda` cigarette butts. 






## Problem 3
Cleaning the `zip_codes_nyc` data set. FIPS is Federal Information Processing System Codes for States and Counties. 
```{r collapse=TRUE}
zip_codes_nyc = read.csv("./data/zillow_data/zip_codes.csv") |> 
  janitor::clean_names()|> 
  select(-state_fips, -file_date) |> 
  relocate(county_code, county_fips, county)
```
It makes sense that the state FIPS code is the same for all zip codes in the state of NYC (all are in New York). I verified this by looking at all of the unique `state_fips` (there is 1). It also happens to be that every observation has the same `file_date` of `7/25/07`. After a brief search on Google, I'm assuming that these zip codes were filed in the early days of Zillow, which launched in 2006. These variables are not really relevant to the data set, so I'm going to exclude them from the working data set. I'm also moving the county code/FIPS and county name to the front of the data set to have the more broad categories on the left.

What's interesting with this data set is that while there are `r nrow(zip_codes_nyc)` observations, but only `r length(unique(zip_codes_nyc$zip_code))` unique zip codes. This means that there are 2 zip codes that are repeated in the data set. 
```{r collapse=TRUE}
repeated_zip_codes = zip_codes_nyc |> 
  group_by(zip_code) |> 
  count(zip_code) |> 
  filter(n > 1) |> 
  pull(zip_code)
```
The two repeated zip codes are `r repeated_zip_codes`. 
```{r collapse=TRUE}
zip_codes_nyc |> 
  filter(zip_code %in% repeated_zip_codes) |> 
  arrange(zip_code)
```
The zipcode 10463 for the Kingsbridge and Riverdale neighborhood appears twice, once in the Bronx and once in New York. The zipcode 11201 for the Northwest Brooklyn neighborhood appears twice, once in Kings and once in New York. 

Moving onto the Zillow Observed Rent Indices (ZORI) index data set. 
```{r}
zori_nyc = read.csv("./data/zillow_data/Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv") |>
  janitor::clean_names() |>
  pivot_longer(
    x2015_01_31:x2024_08_31,
    names_to = "date",
    names_prefix = "x",
    values_to = "index"
  ) |>
  separate("date", into = c("year", "month", "day"), sep = "_", convert = TRUE) |>
  mutate(
    month =
      recode(
        month,
        `1` = "January",
        `2` = "February",
        `3` = "March",
        `4` = "April",
        `5` = "May",
        `6` = "June",
        `7` = "July",
        `8` = "August",
        `9` = "September",
        `10` = "October",
        `11` = "November",
        `12` = "December"
        ),
    month = factor(month, levels = month_order),
    county = str_replace(county_name, " County", ""),
    zip_code = region_name
  ) |>
  select(-county_name, -region_type, -region_name, -state_name, -state, -city, -metro) |> 
  relocate(year, month, day, county, zip_code, region_id, size_rank, index)
```
First, I see that all of the dates with recorded ZORI is spread across a bunch of columns. This can be consolidated into a single `date` variable with the given index or `NA`. It looks like every `region_id` has some missing values, so there will be lots of `NA` indices. 

Like in previous problems, I'm separating the date into `year`, `month`, and `day`, and setting month to be a factor so the default order is chronological, not alphabetical. 

In the formatting of the original data set, I saw that there are 149 observations and each observation corresponds to a unique `region_id`, `size_rank`, `region_name`, and I want to keep these (or some version of these) in my final data set. 

The `county_name` variable lists the `r length(unique(pull(zori_nyc, county)))` counties in NYC as __ County (e.g. Kings County). To match the style for county names in the `zip_codes_nyc` data set, I'm truncating "County" from each name. 

Every `region_type` is `"zip"`, so I'm renaming `region_name` to `zip_code` to match the style for the zip codes in the `zip_codes_nyc` data set. 

For studies looking at multiple states, cities, etc, the variables `state_name`, `state`, `city`, and `metro` would be useful, but since this study is focusing on New York City, keeping these variables seems repetitive. By including `nyc` in the data frame names, I'm effectively communicating the same information as these variables (i.e. someone unfamiliar with the data would be able to tell that this is data from New York City).

So now, we have `zip_codes_nyc`, which has information about counties in NYC (and associated FIPS code and county code), zip codes, and neighborhoods. It has `r nrow(zip_codes_nyc)` observations. 


We also have `zori_nyc`, which also has information about counties in NYC, the zip codes, region IDs, and size ranks, as well as the ZORI at each date recorded. It has `r nrow(zori_nyc)` observations. 
```{r collapse=TRUE}
colnames(zip_codes_nyc)
colnames(zori_nyc)
```

When it comes to joining these two data sets, I have to keep in mind that simply joining on `zip_code` will not suffice, since 2 of the zip codes show up in multiple counties. Since I am primarily interested in the ZORI data (with additional information on neighborhood), a left join on county and ZIP code makes the most sens. 
```{r collapse=TRUE}
zori_nyc_neighborhoods = left_join(zori_nyc, zip_codes_nyc, by = c("zip_code", "county")) |> 
  relocate(year, month, day, county, county_code, county_fips, neighborhood, size_rank, region_id, zip_code) |> 
  arrange(year, month, day)
colnames(zori_nyc_neighborhoods)
```

Looking at the final data set, we can see that there are `r ncol(zori_nyc_neighborhoods)` variables, which corresponds to the `r ncol(zori_nyc)` variables in `zori_nyc` plus the `county_code`, `county_fips`, and `neighborhood` variables found in `zip_codes_nyc` data set. The resulting data set is set in chronological order (putting the date-related variables at the beginning). After the date variables comes the variables related to the county (the name, code, and FIPS code). After county comes the neighborhood, followed by zip code related variables (size_rank, ID, zip code). The size rank is the city's rank by population size.

There are `r nrow(zori_nyc_neighborhoods)` total observations in `zori_nyc_neighborhoods`, with `r length(unique(pull(zori_nyc_neighborhoods, zip_code)))` unique ZIP codes and `r length(unique(pull(zori_nyc_neighborhoods, neighborhood)))` unique neighborhoods.

```{r}
zip_codes_not_in_zori = anti_join(zip_codes_nyc, zori_nyc, by = c("county", "zip_code")) 

codes_with_neighborhood_not_in_zori = zip_codes_not_in_zori |> 
  filter(!is.na(neighborhood))
bronx_neighborhoods = codes_with_neighborhood_not_in_zori |> filter(county == "Bronx") 
kings_neighborhoods = codes_with_neighborhood_not_in_zori |> filter(county == "Kings") 
newyork_neighborhoods = codes_with_neighborhood_not_in_zori |> filter(county == "New York") 
queens_neighborhoods = codes_with_neighborhood_not_in_zori |> filter(county == "Queens") 
richmond_neighborhoods = codes_with_neighborhood_not_in_zori |> filter(county == "Richmond") 
```
There are `r length(unique(pull(zip_codes_not_in_zori, zip_code)))` ZIP codes that appear in `zip_codes_nyc` but not `zori_nyc`. Of these, `r sum(is.na(pull(zip_codes_not_in_zori, neighborhood)))` ZIP codes have no associated neighborhood. The remaining `r length(unique(pull(zip_codes_not_in_zori, zip_code))) - sum(is.na(pull(zip_codes_not_in_zori, neighborhood)))` ZIP codes are in `r length(unique(pull(codes_with_neighborhood_not_in_zori, neighborhood)))` distinct neighborhoods across all 5 counties:

  *    **Bronx** - `r unique(pull(bronx_neighborhoods, neighborhood))`
  *    **Kings** - `r unique(pull(kings_neighborhoods, neighborhood))`
  *    **New York** - `r unique(pull(newyork_neighborhoods, neighborhood))`
  *    **Queens** - `r unique(pull(queens_neighborhoods, neighborhood))`
  *    **Richmond** - `r unique(pull(richmond_neighborhoods, neighborhood))`
  
  
# Add a few illustrative examples discuss why these ZIP codes might be excluded from the Zillow dataset.

```{r}
top10_rental_price_change = zori_nyc_neighborhoods |> 
  filter(month == "January", year %in% c(2020, 2021), !is.na(index), !is.na(neighborhood)) |>
  group_by(zip_code) |> 
  filter(n() == 2) |> 
  mutate(
    date = paste(tolower(month), day, year, sep = "_")
  ) |> 
  select(-year, -month, -day) |> 
  pivot_wider(
    names_from = "date", 
    values_from = "index"
  ) |> 
  summarize(
    county = county,
    neighborhood = neighborhood,
    zip_code = zip_code,
    index_change_covid = january_31_2021 - january_31_2020
  ) |> 
  arrange(index_change_covid) |> 
  slice(1:10) 
knitr::kable(top10_rental_price_change)
```
All of the biggest drops are in New York County, specifically in downtown Manhattan. The neighborhoods are `r unique(pull(top10_rental_price_change, neighborhood))`. These are all typically more expensive and have a large amount of rental properties (as compared to other neighborhoods that will have more properties that are occupied by the owner of the property)
